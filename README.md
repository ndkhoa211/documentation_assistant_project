# Documentation Assistant **for LangChain**

> A selfâ€‘contained chatbot that indexes the entire LangChain Python documentation, stores it in Pinecone, and lets you query it conversationally with source citations.

![Python](https://img.shields.io/badge/python-3.12%2B-blue?logo=python)
![Streamlit](https://img.shields.io/badge/Streamlit-1.46-red?logo=streamlit)
![LangChain](https://img.shields.io/badge/LangChain-0.3.x-9cf?logo=langchain)
[![LangSmith](https://img.shields.io/badge/LangSmith-enabled-brightgreen)](https://smith.langchain.com/o/856312b1-7816-4389-80cb-b01e398655be/projects/p/29eae9f5-17ef-4946-a14b-58a9570b274e?timeModel=%7B%22duration%22%3A%227d%22%7D)
[![Pinecone](https://img.shields.io/badge/Pinecone-VectorDB-blueviolet)](https://app.pinecone.io/organizations/-OQOYTa7PD5A_F9pFlNC/projects/0fed2f10-ab48-4302-a34b-583868c23c78/indexes/documentation-assistant-project/browser)
[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-langchain__documentation__assistant-yellow?logo=huggingface)](https://huggingface.co/spaces/ndk211/langchain_documentation_assistant)
![License](https://img.shields.io/badge/license-MIT-lightgrey)
---

## What I built ðŸ”

* **Fullâ€‘text crawler & indexer**Â â€“ loads the LangChain Readâ€‘theâ€‘Docs site, chunks each page to Â±600Â chars, cleans the URLs and writes the embeddings to **Pinecone** using *`textâ€‘embeddingâ€‘3â€‘small`*Â ([raw.githubusercontent.com](https://raw.githubusercontent.com/ndkhoa211/documentation_assistant_project/main/ingestion.py))
* **Historyâ€‘aware retrieval chain**Â â€“ wraps a GPTâ€‘4.1 chat model with LangChainâ€™s *historyâ€‘aware retriever* + *retrievalâ€‘qaâ€‘chat* prompts for grounded answers.Â ([raw.githubusercontent.com](https://raw.githubusercontent.com/ndkhoa211/documentation_assistant_project/main/backend/core.py))
* **Streamlit frontâ€‘end**Â â€“ a oneâ€‘file UI (`main.py`) with a profile sidebar, chat bubbles, and clickable source links.Â ([raw.githubusercontent.com](https://raw.githubusercontent.com/ndkhoa211/documentation_assistant_project/main/main.py))
* **Reâ€‘usable project template**Â â€“ dependencyâ€‘pinned **pyproject.toml** (uvâ€‘style), theming via `.streamlit/config.toml`, and a deployâ€‘ready HFÂ Spaces repo.Â ([raw.githubusercontent.com](https://raw.githubusercontent.com/ndkhoa211/documentation_assistant_project/main/pyproject.toml), [raw.githubusercontent.com](https://raw.githubusercontent.com/ndkhoa211/documentation_assistant_project/main/.streamlit/config.toml))

---

## QuickÂ demo

```bash
# 1Â Clone & enter
$ git clone https://github.com/ndkhoa211/documentation_assistant_project
$ cd documentation_assistant_project

# 2Â Create isolated env (uv recommended)
$ uv venv            # â†’ .venv/
$ source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 3Â Install runtime deps + dev extras
$ uv pip install -e .

# 4Â Set your secrets (OpenAI + Pinecone)
$ cat > .env <<'EOF'
OPENAI_API_KEY=skâ€‘...
PINECONE_API_KEY=...
PINECONE_ENV=usâ€‘eastâ€‘1â€‘aws
EOF

# 5Â Oneâ€‘off ingestion (â‰ˆÂ 2â€“3Â min)
$ python ingestion.py

# 6Â Launch the chat app
$ streamlit run main.py
```

> **Tip**Â â€“ Skip stepsÂ 2â€‘6 entirely by hitting theÂ ðŸ¤— **Spaces** badge above. (CURRENTLY MADE PRIVATE)

---

## How it works

### 1. Ingestion pipeline

1. **ReadTheDocsLoader** pulls every `.html` page from the local mirror `langchain-docs/`.
2. `RecursiveCharacterTextSplitter` cuts pages into 600â€‘character overlaps to fit the 8Â k context window.
3. Each chunk is embedded with **OpenAI `textâ€‘embeddingâ€‘3â€‘small`**, then upserted in batches ofÂ 100 to **Pinecone** (`documentation-assistant-project` index).
4. Canonical HTTPS URLs are patched into `Document.metadata['source']` so users can click through.

### 2. Retrievalâ€‘augmented generation

* On each user prompt:

  * **Historyâ€‘aware retriever** rewrites the question given the last turns.
  * Topâ€‘k documents (k=4 default) are fetched from Pinecone.
  * GPTâ€‘4.1 is run with the `retrieval-qa-chat` prompt and stuffed context.
* The response is streamed back with numbered source links.

### 3. Streamlit UI

The chat frontâ€‘end lives entirely in `main.py` and offers:

* Persistent *`st.session_state`* chat history.
* (Made with Cursor) A sidebar showing user initials, contact info, and project branding.
* (Made with Cursor) Pytorch-alike color palette configured in `.streamlit/config.toml`.

---

## Repository structure

```text
documentation_assistant_project/
â”œâ”€â”€ backend/
â”‚Â Â  â”œâ”€â”€ core.py        # retrieval chain logic
â”‚Â Â  â””â”€â”€ __init__.py
â”œâ”€â”€ langchain-docs/    # offline copy of docs (gitâ€‘ignored in remote)
â”œâ”€â”€ ingestion.py       # crawl âžœ chunk âžœ embed âžœ upsert
â”œâ”€â”€ main.py            # Streamlit app
â”œâ”€â”€ .streamlit/
â”‚Â Â  â””â”€â”€ config.toml    # theme + static serving
â”œâ”€â”€ pyproject.toml     # deps pinned for uv
â””â”€â”€ uv.lock            # exact versions
```

---

## Environment variables

| Variable            | Purpose                       |
|---------------------|-------------------------------|
| `OPENAI_API_KEY`    | LLM + embeddings              |
| `PINECONE_API_KEY`  | Vector database access        |
| `PINECONE_ENV`      | Region (e.g. `us-east-1-aws`) |
| `INDEX_NAME`        | Vector database index         |
| `LANGSMITH_API_KEY` | LLM calls tracing             |
| `LANGCHAIN_PROJECT` | Documen t Assistant Project   |
| `LANGSMITH_TRACING` | true                          |

Store them in a local `.env` â€“ they are loaded by `pythonâ€‘dotenv` at runtime.

